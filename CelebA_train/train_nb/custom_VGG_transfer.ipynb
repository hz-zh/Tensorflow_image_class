{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels.csv\n",
    "labels_df = pd.read_csv('labels.csv')  # Update with your file path\n",
    "\n",
    "# Load list_bbox_celeba.txt\n",
    "bbox_df = pd.read_csv('list_bbox_celeba.txt', delim_whitespace=True, header=1)  # Update with your file path\n",
    "\n",
    "# Merge dataframes based on the 'filename' column\n",
    "merged_df = labels_df.merge(bbox_df, on='filename')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['x_normalized'] = merged_df['x_1'] / merged_df['width']\n",
    "merged_df['y_normalized'] = merged_df['y_1'] / merged_df['height']\n",
    "merged_df['width_normalized'] = merged_df['width'] / merged_df['width']\n",
    "merged_df['height_normalized'] = merged_df['height'] / merged_df['height']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size, image_size, class_names, bbox_columns):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.class_names = class_names\n",
    "        self.bbox_columns = bbox_columns\n",
    "        self.num_classes = len(class_names)\n",
    "        self.indexes = np.arange(len(dataframe))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        batch_data = self.dataframe.iloc[batch_indexes]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_classes = []\n",
    "        batch_bboxes = []\n",
    "\n",
    "        for _, row in batch_data.iterrows():\n",
    "            image = cv2.imread(row['filename'])  # Load image using OpenCV\n",
    "            image = cv2.resize(image, self.image_size)  # Resize to desired size\n",
    "\n",
    "            bbox_normalized = [row[col] for col in self.bbox_columns]  # Get normalized bbox coordinates\n",
    "\n",
    "            class_vector = [row[class_name] for class_name in self.class_names]  # Get class labels\n",
    "\n",
    "            batch_images.append(image)\n",
    "            batch_classes.append(class_vector)\n",
    "            batch_bboxes.append(bbox_normalized)\n",
    "\n",
    "        return (\n",
    "            np.array(batch_images),\n",
    "            {'class_names': np.array(batch_classes), 'bboxes': np.array(batch_bboxes)}\n",
    "        )\n",
    "\n",
    "# List of class names\n",
    "class_names = labels_df.columns[1:].tolist()\n",
    "\n",
    "# List of bbox columns\n",
    "bbox_columns = ['x_normalized', 'y_normalized', 'width_normalized', 'height_normalized']\n",
    "\n",
    "# Image size\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Create data generator\n",
    "batch_size = 32\n",
    "data_generator = CustomImageDataGenerator(merged_df, batch_size, image_size, class_names, bbox_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flow = data_generator.flow_from_dataframe(\n",
    "    merged_df,\n",
    "    x_col='filename',\n",
    "    y_col={'class_names': class_names, 'bboxes': bbox_columns},\n",
    "    batch_size=batch_size,\n",
    "    target_size=image_size,\n",
    "    class_mode='raw',  # We will use 'raw' mode to pass custom target data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_and_bboxes(images, bboxes, target_size):\n",
    "    cropped_images = []\n",
    "    for image, bbox in zip(images, bboxes):\n",
    "        x, y, width, height = bbox\n",
    "        x_min = int(x * image.shape[1])\n",
    "        y_min = int(y * image.shape[0])\n",
    "        x_max = int((x + width) * image.shape[1])\n",
    "        y_max = int((y + height) * image.shape[0])\n",
    "        \n",
    "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "        resized_image = cv2.resize(cropped_image, target_size)\n",
    "        \n",
    "        cropped_images.append(resized_image)\n",
    "    \n",
    "    return np.array(cropped_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "roi_input = Input(shape=(None, None, 3))  # Shape can be adjusted based on your bounding box sizes\n",
    "roi_features = base_model(roi_input)\n",
    "global_avg_pooling = GlobalAveragePooling2D()(roi_features)\n",
    "class_output = Dense(num_classes, activation='softmax')(global_avg_pooling)\n",
    "bbox_output = Dense(4, activation='linear')(global_avg_pooling)\n",
    "\n",
    "model = Model(inputs=roi_input, outputs=[class_output, bbox_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'class_output': 'categorical_crossentropy', 'bbox_output': 'mean_squared_error'},\n",
    "    loss_weights={'class_output': 1.0, 'bbox_output': 1.0},\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_images, batch_targets in data_flow:\n",
    "        cropped_images = preprocess_images_and_bboxes(batch_images, batch_targets['bboxes'], target_size=(224, 224))\n",
    "        \n",
    "        class_targets = batch_targets['class_names']\n",
    "        bbox_targets = batch_targets['bboxes']\n",
    "        \n",
    "        model.fit(\n",
    "            cropped_images,\n",
    "            {'class_output': class_targets, 'bbox_output': bbox_targets},\n",
    "            batch_size=batch_size\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
